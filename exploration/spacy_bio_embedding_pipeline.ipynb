{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "final-mailing",
   "metadata": {},
   "source": [
    "# Model details\n",
    "- Trained with 685k unique vectors (words) with 300 dimensions each\n",
    "- Language data was from OntoNotes - news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows\n",
    "    - https://catalog.ldc.upenn.edu/LDC2013T19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-feeding",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-bronze",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from autocorrect import Speller\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tweepy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-netscape",
   "metadata": {},
   "source": [
    "# Get bios from Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Uroihphf25vaDmGPadvG1klrU\"\n",
    "api_secret_key = \"mpQ6yDaGmdiyPW6ytzEo5nrw8SZn94YZKtMnx3tx13RQj7HoYs\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAORhNAEAAAAAR1ODpBlt9PHGImWMWE8WxxfRHhY%3D0UDIDDI9ARsdZzFo4Pf7jrVdlIdzgqz1CDWcoYCGdm42jieyLR\"\n",
    "access_token = \"889954860329730048-7mBJQPfciA1pva5Olpovd22deE8hplo\"\n",
    "access_token_secret = \"FGy9LvHHZt6X6W1umPugGlgPJsdWpUjH9Eu4re9ozxNEz\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-meditation",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"neuroscience\",\n",
    "    \"entertainment\",\n",
    "    \"finance\",\n",
    "    \"fitness\"\n",
    "]\n",
    "usernames = np.unique([user.screen_name for q in queries for user in api.search_users(q)])\n",
    "profiles = [api.get_user(user).description for user in usernames]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-saturn",
   "metadata": {},
   "source": [
    "# Process bio data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_char(char: str, sentence: str):\n",
    "    return \" \".join(filter(lambda x:x[0]!=char, sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, conver to lower case, remove non words and stop words, autocorrect\n",
    "spell = Speller()\n",
    "stop_words = np.unique(stopwords.words('english') + [\"the\"])\n",
    "proc_profiles = []\n",
    "for prof in profiles:\n",
    "    # remove anything with @ in front\n",
    "    tokens = wordpunct_tokenize(remove_words_with_char(\"@\",prof))\n",
    "    processed_profile = []\n",
    "    for word in tokens:\n",
    "        cleaned_word = spell(word.lower())\n",
    "        if cleaned_word.isalpha() and cleaned_word not in stop_words:\n",
    "            processed_profile.append(cleaned_word)\n",
    "    proc_profiles.append(processed_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-omega",
   "metadata": {},
   "source": [
    "# Generate bio embeddings with spacy's model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-brisbane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# turn tokenized profiles back into full sentence to embed\n",
    "full_profiles = [\" \".join(prof) for prof in proc_profiles]\n",
    "# calculate average word embedding for each profile\n",
    "ave_embeddings = np.array([nlp(prof).vector for prof in full_profiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize ave. profile embeddings\n",
    "pca = PCA(n_components=2)\n",
    "embed_2d = pca.fit_transform(ave_embeddings)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(embed_2d[:, 0], embed_2d[:, 1])\n",
    "for i, user in enumerate(usernames):\n",
    "    plt.annotate(user, xy=(embed_2d[i, 0], embed_2d[i, 1]))\n",
    "plt.xlabel(\"dimension 1\",fontsize=16)\n",
    "plt.ylabel(\"dimension 2\",fontsize=16)\n",
    "plt.title(\"Average Profile Embedding -- SpaCy Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "positive-collaboration",
   "metadata": {},
   "source": [
    "# Cluster profiles based on category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bearing-fever",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster embeddings based on category\n",
    "kmeans = KMeans(n_clusters=len(queries), random_state=0)\n",
    "preds = kmeans.fit_predict(ave_embeddings)\n",
    "\n",
    "color_map = {i:c for i,c in zip(range(len(queries)),[\"red\",\"blue\",\"orange\",\"green\"])}\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "for i, pred in enumerate(preds):\n",
    "    plt.scatter(embed_2d[i, 0], embed_2d[i, 1], color=color_map[pred])\n",
    "    plt.annotate(pred, xy=(embed_2d[i, 0], embed_2d[i, 1]))\n",
    "plt.xlabel(\"dimension 1\",fontsize=16)\n",
    "plt.ylabel(\"dimension 2\",fontsize=16)\n",
    "plt.title(\"Average Profile Embedding with KMeans Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-producer",
   "metadata": {},
   "source": [
    "# Compute similarity between users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity\n",
    "cos_similarity = cosine_similarity(ave_embeddings)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(cos_similarity,cmap=\"Blues\",xticklabels=usernames,yticklabels=usernames)\n",
    "plt.title(\"Cosine similarity - Ave. Profile Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-pavilion",
   "metadata": {},
   "source": [
    "# Create recommendations based on similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-milan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rankings(users, similarity_scores, num_recs: int=5) -> pd.DataFrame:\n",
    "    user_recs_dict = {\"user\":[],\"recommendation\":[],\"score\":[], \"ranking\":[]}\n",
    "    for i,user in enumerate(users):\n",
    "        # get sim scores and remove current user\n",
    "        curr_sim_scores = np.delete(similarity_scores[i].copy(),i)\n",
    "        curr_users = np.delete(np.array(users.copy()),i)\n",
    "        # get recs\n",
    "        for i in range(num_recs):\n",
    "            user_recs_dict[\"user\"].append(user)\n",
    "            # current most similar user and score\n",
    "            top_sim_idx = np.argmax(curr_sim_scores)\n",
    "            top_sim_user = curr_users[top_sim_idx]\n",
    "            top_sim_score = curr_sim_scores[top_sim_idx]\n",
    "            user_recs_dict[\"recommendation\"].append(top_sim_user)\n",
    "            user_recs_dict[\"score\"].append(top_sim_score)\n",
    "            user_recs_dict[\"ranking\"].append(i+1)\n",
    "            # remove from lists\n",
    "            curr_sim_scores = np.delete(curr_sim_scores,top_sim_idx)\n",
    "            curr_users = np.delete(curr_users,top_sim_idx)\n",
    "\n",
    "    user_recs = pd.DataFrame.from_dict(user_recs_dict)\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recs = similarity_rankings(usernames, cos_similarity)\n",
    "rand_users = np.random.choice(usernames,size=8,replace=False)\n",
    "fig,axes = plt.subplots(4,2,figsize=(15,12),sharey=True)\n",
    "for i,(user,ax) in enumerate(zip(rand_users,np.ravel(axes))):\n",
    "    curr_user = user_recs[user_recs[\"user\"]==user]\n",
    "    ax.bar(curr_user.recommendation,curr_user.score)\n",
    "    ax.set_title(user)\n",
    "    ax.set_ylabel(\"Cosine Similarity\") if not i%2 else ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-campbell",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
