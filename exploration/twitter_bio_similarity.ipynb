{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "raised-evaluation",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-monday",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-tourist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tweepy\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from autocorrect import Speller\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "median-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-chosen",
   "metadata": {},
   "source": [
    "# Set up Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"Uroihphf25vaDmGPadvG1klrU\"\n",
    "api_secret_key = \"mpQ6yDaGmdiyPW6ytzEo5nrw8SZn94YZKtMnx3tx13RQj7HoYs\"\n",
    "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAORhNAEAAAAAR1ODpBlt9PHGImWMWE8WxxfRHhY%3D0UDIDDI9ARsdZzFo4Pf7jrVdlIdzgqz1CDWcoYCGdm42jieyLR\"\n",
    "access_token = \"889954860329730048-7mBJQPfciA1pva5Olpovd22deE8hplo\"\n",
    "access_token_secret = \"FGy9LvHHZt6X6W1umPugGlgPJsdWpUjH9Eu4re9ozxNEz\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-moral",
   "metadata": {},
   "source": [
    "# Grab user profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use multiple categories so that we can actually see the efficacy of our rec system\n",
    "queries = [\n",
    "    \"neuroscience\",\n",
    "    \"machine learning\",\n",
    "    \"education\",\n",
    "    \"medicine\",\n",
    "    \"movies\",\n",
    "    \"entertainment\",\n",
    "    \"finance\",\n",
    "    \"business\",\n",
    "    \"fitness\"\n",
    "]\n",
    "n_profs = 3\n",
    "username_map = {}\n",
    "for q in queries:\n",
    "    username_map[q] = np.random.choice(np.unique([user.screen_name for user in api.search_users(q)]),size=3,replace=False)\n",
    "\n",
    "# get profiles\n",
    "profiles = []\n",
    "for cat in username_map:\n",
    "    users = username_map[cat]\n",
    "    for user in users:\n",
    "        try:\n",
    "            profiles.append(api.get_user(user).description)\n",
    "        except:\n",
    "            print(f\"Error for user {name}, removing\")\n",
    "            username_map[cat].remove(user)\n",
    "            \n",
    "# all usernames\n",
    "usernames = np.ravel(list(username_map.values()))\n",
    "\n",
    "# user-category pairs\n",
    "cat_map = {user:cat for cat in username_map for user in usernames if user in username_map[cat]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-parker",
   "metadata": {},
   "source": [
    "# Process profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impressive-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_with_char(char: str, sentence: str):\n",
    "    return \" \".join(filter(lambda x:x[0]!=char, sentence.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize, conver to lower case, remove non words and stop words, autocorrect\n",
    "spell = Speller()\n",
    "stop_words = np.unique(stopwords.words('english') + [\"the\"])\n",
    "proc_profiles = []\n",
    "for prof in profiles:\n",
    "    # remove anything with @ in front\n",
    "    tokens = wordpunct_tokenize(remove_words_with_char(\"@\",prof))\n",
    "    processed_profile = []\n",
    "    for word in tokens:\n",
    "        cleaned_word = spell(word.lower())\n",
    "        if cleaned_word.isalpha() and cleaned_word not in stop_words:\n",
    "            processed_profile.append(cleaned_word)\n",
    "    proc_profiles.append(processed_profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-dream",
   "metadata": {},
   "source": [
    "# Explore profile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean length of profile:\", int(np.mean([len(prof) for prof in proc_profiles])), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most common words\n",
    "all_words = np.hstack(proc_profiles)\n",
    "freq = nltk.FreqDist(all_words)\n",
    "sorted_freq = {k: v for k, v in sorted(freq.items(), key=lambda item: item[1])[::-1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-panama",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 8\n",
    "fig,ax = plt.subplots(1,1,figsize=(9,6))\n",
    "ax.bar(np.arange(num_words), list(sorted_freq.values())[:num_words])\n",
    "ax.set_xticks(np.arange(num_words))\n",
    "ax.set_xticklabels(list(sorted_freq.keys())[:num_words])\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"Most common words from {len(proc_profiles)} profiles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique words\n",
    "print(\"# of unique words:\",len(set(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-engagement",
   "metadata": {},
   "source": [
    "# One-hot vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharing-chair",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_dict = {}\n",
    "for word in all_words:\n",
    "    onehot_dict[word] = np.zeros(len(usernames))\n",
    "    \n",
    "onehot_df = pd.DataFrame(onehot_dict,index=usernames)\n",
    "for prof,(idx,row) in zip(proc_profiles,onehot_df.iterrows()):\n",
    "    for word in prof:\n",
    "        onehot_df.loc[idx,word] = 1\n",
    "\n",
    "onehot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "sns.heatmap(onehot_df,cmap=\"Blues\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-makeup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity\n",
    "cos_similarity = cosine_similarity(onehot_df.to_numpy())\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(cos_similarity,cmap=\"Blues\",xticklabels=usernames,yticklabels=usernames)\n",
    "plt.title(\"Cosine similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-artist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_rankings(users, similarity_scores, num_recs: int=5) -> pd.DataFrame:\n",
    "    user_recs_dict = {\"user\":[],\"recommendation\":[],\"score\":[], \"ranking\":[]}\n",
    "    for i,user in enumerate(users):\n",
    "        # get sim scores and remove current user\n",
    "        curr_sim_scores = np.delete(similarity_scores[i].copy(),i)\n",
    "        curr_users = np.delete(np.array(users.copy()),i)\n",
    "        # get recs\n",
    "        for i in range(num_recs):\n",
    "            user_recs_dict[\"user\"].append(user)\n",
    "            # current most similar user and score\n",
    "            top_sim_idx = np.argmax(curr_sim_scores)\n",
    "            top_sim_user = curr_users[top_sim_idx]\n",
    "            top_sim_score = curr_sim_scores[top_sim_idx]\n",
    "            user_recs_dict[\"recommendation\"].append(top_sim_user)\n",
    "            user_recs_dict[\"score\"].append(top_sim_score)\n",
    "            user_recs_dict[\"ranking\"].append(i+1)\n",
    "            # remove from lists\n",
    "            curr_sim_scores = np.delete(curr_sim_scores,top_sim_idx)\n",
    "            curr_users = np.delete(curr_users,top_sim_idx)\n",
    "\n",
    "    user_recs = pd.DataFrame.from_dict(user_recs_dict)\n",
    "    return user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-verse",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_user_recs = similarity_rankings(usernames, cos_similarity)\n",
    "rand_users = np.random.choice(usernames,size=8,replace=False)\n",
    "fig,axes = plt.subplots(4,2,figsize=(15,12),sharey=True)\n",
    "for i,(user,ax) in enumerate(zip(rand_users,np.ravel(axes))):\n",
    "    curr_user = cos_user_recs[cos_user_recs[\"user\"]==user]\n",
    "    recs = curr_user.recommendation\n",
    "    recs_w_cat = [rec_user+f\"\\n({cat_map[rec_user]})\" for rec_user in recs]\n",
    "    ax.bar(recs_w_cat,curr_user.score)\n",
    "    ax.set_title(f\"{user} ({cat_map[user]})\")\n",
    "    ax.set_ylabel(\"cosine similarity\") if not i%2 else ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-creation",
   "metadata": {},
   "source": [
    "# SpaCy word embedding model\n",
    "- trained on 685 unique vectors with 300 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-calibration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn tokenized profiles back into full sentence to embed\n",
    "full_profiles = [\" \".join(prof) for prof in proc_profiles]\n",
    "\n",
    "# calculate average word embedding for each profile\n",
    "ave_embeddings = np.array([nlp(prof).vector for prof in full_profiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute similarity\n",
    "embed_cos_similarity = cosine_similarity(ave_embeddings)\n",
    "plt.figure(figsize=(15,12))\n",
    "sns.heatmap(embed_cos_similarity,cmap=\"Blues\",xticklabels=usernames,yticklabels=usernames)\n",
    "plt.title(\"Cosine similarity - Ave. Profile Embedding\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-instruction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize ave. profile embeddings\n",
    "pca = PCA(n_components=2)\n",
    "embed_2d = pca.fit_transform(ave_embeddings)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(embed_2d[:, 0], embed_2d[:, 1])\n",
    "for i, user in enumerate(usernames):\n",
    "    plt.annotate(user, xy=(embed_2d[i, 0], embed_2d[i, 1]))\n",
    "plt.xlabel(\"dimension 1\",fontsize=16)\n",
    "plt.ylabel(\"dimension 2\",fontsize=16)\n",
    "plt.title(\"Average Profile Embedding -- SpaCy Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-wichita",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pairs = pd.DataFrame({\n",
    "    \"user1\":[],\n",
    "    \"user2\":[],\n",
    "    \"profile1\":[],\n",
    "    \"profile2\":[],\n",
    "    \"category1\":[],\n",
    "    \"category2\":[],\n",
    "    \"similarity\":[]\n",
    "})\n",
    "for user,prof,embed in zip(usernames,full_profiles,ave_embeddings):\n",
    "    for other_user,other_prof,other_embed in zip(usernames,profiles,ave_embeddings):\n",
    "        if (other_user == user):\n",
    "            continue\n",
    "        new_row = pd.DataFrame()\n",
    "        new_row[\"user1\"] = [user]\n",
    "        new_row[\"user2\"] = [other_user]\n",
    "        new_row[\"profile1\"] = [prof]\n",
    "        new_row[\"profile2\"] = [other_prof]\n",
    "        new_row[\"category1\"] = [cat_map[user]]\n",
    "        new_row[\"category2\"] = [cat_map[other_user]]\n",
    "        new_row[\"similarity\"] = [cosine_similarity(embed.reshape(1,-1),other_embed.reshape(1,-1))[0].item()]\n",
    "        all_pairs = all_pairs.append(new_row,ignore_index=True)\n",
    "all_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "fig,axes = plt.subplots(5,2,figsize=(18,18),sharey=True)\n",
    "for i,(cat,ax) in enumerate(zip(queries,np.ravel(axes))):\n",
    "    sns.barplot(data=all_pairs[all_pairs[\"category1\"]==cat],x=\"category2\",y=\"similarity\",ax=ax)\n",
    "    ax.set_title(cat)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45,fontsize=12)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Cosine Similarity\",fontsize=14) if not i%2 else ax.set_ylabel(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = usernames[0]\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "sns.barplot(data=all_pairs[all_pairs[\"user1\"]==user],x=\"category2\",y=\"similarity\")\n",
    "plt.title(f\"User -- {user} ({cat_map[user]})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-geneva",
   "metadata": {},
   "source": [
    "# Google News Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-platform",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-facial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-frank",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-platinum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-division",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-stupid",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
